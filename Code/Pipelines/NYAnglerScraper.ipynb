{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    " logging.basicConfig(level=logging.ERROR,\n",
    "                            format=\"%(asctime)s %(levelname)s %(threadName)s %(name)s %(message)s\",\n",
    "                            filename='acquirefunc.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYAngler Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyanger_url = 'https://nyangler.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/desbrium/Metis/FishingSpots/Data/NyAngler/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(nyanger_url + '/forums/')\n",
    "page = response.text\n",
    "soup = BeautifulSoup(page,'lxml')\n",
    "\n",
    "forum_threads = soup.find_all('div', class_ = 'block-container')\n",
    "saltwater_threads = forum_threads[3].find_all('h3', class_ = 'node-title')\n",
    "freshwater_threads = forum_threads[4].find_all('h3', class_ = 'node-title')\n",
    "\n",
    "forum_link_dict = {'Thread_Name':['Fishing Report', 'New York State DEC Updates', 'New York Fishing Podcast'], \n",
    "                   'Thread_Link':['/forums/fishing-report.93/', '/forums/new-york-state-dec-updates.91/', '/forums/new-york-fishing-podcast.68/'],\n",
    "                   'Thread_Category':['Information', 'Information', 'Information']}\n",
    "\n",
    "forum_link_dict['Thread_Link'].extend([thread.find('a').get('href') for thread in saltwater_threads])\n",
    "forum_link_dict['Thread_Name'].extend([thread.text.replace('New', '').strip() for thread in saltwater_threads])\n",
    "forum_link_dict['Thread_Category'].extend(['SaltWater_Fishing' for thread in saltwater_threads])\n",
    "\n",
    "forum_link_dict['Thread_Link'].extend([thread.find('a').get('href') for thread in freshwater_threads])\n",
    "forum_link_dict['Thread_Name'].extend([thread.text.replace('New', '').strip() for thread in freshwater_threads])\n",
    "forum_link_dict['Thread_Category'].extend(['FreshWater_Fishing' for thread in freshwater_threads])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_link_dict = {'Thread_Name': [], 'Page_Links':[]}\n",
    "\n",
    "link_indexes = len(forum_link_dict['Thread_Link'])\n",
    "thread_links = forum_link_dict['Thread_Link']\n",
    "thread_names = forum_link_dict['Thread_Name']\n",
    "\n",
    "for index in range(link_indexes):\n",
    "    \n",
    "    thread_link = thread_links[index]\n",
    "    thread_name = thread_names[index]\n",
    "    \n",
    "    response = requests.get(f'{nyanger_url + thread_link}')\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "    \n",
    "    try: \n",
    "        \n",
    "        last_page = soup.find_all('li', class_ = 'pageNav-page')[-1].text\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        continue \n",
    "        \n",
    "    for page_num in range(1,int(last_page)+1):\n",
    "        \n",
    "        if thread_link not in thread_link_dict['Page_Links']:\n",
    "            \n",
    "            thread_link_dict['Page_Links'].append(thread_link)\n",
    "            thread_link_dict['Thread_Name'].append(thread_name)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            thread_link_dict['Page_Links'].append(thread_link+f'page-{page_num}')\n",
    "            thread_link_dict['Thread_Name'].append(thread_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_link_dict = {'Thread_Name': [], 'Page_Links':[], 'Post_Links':[]}\n",
    "\n",
    "link_indexes = len(thread_link_dict['Page_Links'])\n",
    "page_links = thread_link_dict['Page_Links']\n",
    "thread_names = thread_link_dict['Thread_Name']\n",
    "\n",
    "for index in range(link_indexes):\n",
    "    \n",
    "    page_link = page_links[index]\n",
    "    thread_name = thread_names[index]\n",
    "    \n",
    "    response = requests.get(f'{nyanger_url + page_link}')\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,'lxml')\n",
    "    \n",
    "    table = soup.find('div', class_ ='structItemContainer')\n",
    "    post_containers = table.find_all('li', class_ = 'structItem-startDate')\n",
    "        \n",
    "    for post in post_containers:\n",
    "        \n",
    "        post_link_dict['Thread_Name'].append(thread_name)\n",
    "        post_link_dict['Page_Links'].append(page_link)\n",
    "        post_link_dict['Post_Links'].append(post.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_post_info(base_url = nyanger_url, post_link = None):\n",
    "        \n",
    "    post_dict = {'Post_Links':'', 'Post_Title': '', 'Post_DateTime': '', 'Post_Content': '', 'Post_Content_Dates': []}\n",
    "\n",
    "    page_links = []\n",
    "    \n",
    "    page_links.append(f'{base_url + post_link}')\n",
    "\n",
    "    response = requests.get(page_links[0])\n",
    "    \n",
    "    if int(response.status_code) < 400:\n",
    "        \n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page,'lxml')\n",
    "\n",
    "        table = soup.find('div', class_ = 'p-body-inner')\n",
    "\n",
    "        post_dict['Post_Links'] += post_link\n",
    "        \n",
    "        post_title = table.find('h1').text \n",
    "        post_dict['Post_Title'] += post_title\n",
    "\n",
    "        try: \n",
    "\n",
    "            last_page_num = int(table.find_all('li', class_ = 'pageNav-page')[-1].text)\n",
    "\n",
    "            if last_page_num > 2:\n",
    "\n",
    "                for page_num in range(2, last_page_num + 1):\n",
    "\n",
    "                    page_links.append(f'{base_url + post_link}' + f'page-{page_num}')\n",
    "\n",
    "            else: \n",
    "\n",
    "                page_links.append(f'{base_url + post_link}' + f'page-2')\n",
    "\n",
    "        except:\n",
    "\n",
    "            pass\n",
    "\n",
    "        for page_link in page_links:\n",
    "\n",
    "            response = requests.get(page_link)\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page,'lxml')\n",
    "\n",
    "            table = soup.find('div', class_ = 'p-body-inner')\n",
    "\n",
    "            try:\n",
    "\n",
    "                for blockquotes in table.find_all('blockquote'):\n",
    "\n",
    "                    blockquotes.decompose()\n",
    "            except:\n",
    "\n",
    "                print(page_link)\n",
    "\n",
    "                pass\n",
    "\n",
    "            posts = table.find_all('div', class_ = 'message-cell message-cell--main')\n",
    "\n",
    "            for post in posts:\n",
    "\n",
    "                post_text = post.find('div', class_ = 'bbWrapper').text.strip()\n",
    "\n",
    "                #logging.error(f'{page_link}{post}')\n",
    "\n",
    "                if len(post_text) > 30:\n",
    "\n",
    "                    post_time = post.find('time').get('title')\n",
    "                    post_dict['Post_Content'] += post_text\n",
    "                    post_dict['Post_Content_Dates'].append(post_time)\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                post_dict['Post_DateTime'] += post_dict['Post_Content_Dates'][0]\n",
    "    \n",
    "            except:\n",
    "                \n",
    "                continue\n",
    "    else:\n",
    "            \n",
    "        print(f'{response.status_code}')\n",
    "    \n",
    "    return post_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nyangler_df(post_links = post_link_dict['Post_Links'], start_num = None, end_num = None, download_folder = data_folder):\n",
    "    \n",
    "    forum_info = []\n",
    "    \n",
    "    for link in post_links[start_num : end_num]:\n",
    "        \n",
    "        forum_info.append(acquire_post_info(post_link = link))\n",
    "    \n",
    "    df = pd.DataFrame(forum_info)\n",
    "    \n",
    "    file_name = f'NYAnglerPostData{start_num}{end_num}{datetime.date.today().strftime(\"%m-%d-%y\")}.pkl'\n",
    "\n",
    "    file_path = os.path.join(data_folder,file_name)\n",
    "    \n",
    "    df.to_pickle(file_path)\n",
    "    \n",
    "    logging.error(f'{start_num}{end_num}')\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_chunks(post_links = post_link_dict['Post_Links']):\n",
    "    \n",
    "    post_chunks = []\n",
    "    \n",
    "    length = len(post_links)\n",
    "    post_bins = np.linspace(0, length, num= length//40, dtype=np.int64)\n",
    "    \n",
    "    for idx, post_bin in enumerate(post_bins):\n",
    "\n",
    "        if idx == len(post_bins) - 1: \n",
    "\n",
    "            break\n",
    "\n",
    "        start_num =  post_bins[idx]\n",
    "        end_num = post_bins[idx+1]\n",
    "        \n",
    "        post_chunks.append((start_num, end_num))\n",
    "        \n",
    "    return post_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post_info(post_links = post_link_dict['Post_Links'], chunk_list = post_chunks(), start_index = None):\n",
    "        \n",
    "        for index,chunk in enumerate(chunk_list[start_index:]):\n",
    "            \n",
    "            start_num, end_num = chunk\n",
    "        \n",
    "            yield create_nyangler_df(start_num = start_num, end_num = end_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "\n",
    "# gen = create_post_info(start_index = 23)\n",
    "\n",
    "# for postinfo in gen:\n",
    "#     data_list.append(postinfo)\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = []\n",
    "\n",
    "# for data in data_list:\n",
    "#     df_list.append(pd.read_pickle(data))\n",
    "\n",
    "# df = pd.concat(df_list, axis = 0)\n",
    "# nyangler_df = pd.DataFrame(post_link_dict).merge(pd.DataFrame(forum_link_dict), how = 'left', on = 'Thread_Name').merge(df, how = 'left', on = 'Post_Links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f'NYAnglerPostData{datetime.date.today().strftime(\"%m-%d-%y\")}.csv'\n",
    "\n",
    "# file_path = os.path.join(data_folder,file_name)\n",
    "\n",
    "# nyangler_df.to_csv(file_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'/home/desbrium/Metis/FishingSpots/Data/NyAngler/NYAnglerPostData06-22-21.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
