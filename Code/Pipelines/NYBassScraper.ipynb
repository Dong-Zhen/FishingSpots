{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    " logging.basicConfig(level=logging.ERROR,\n",
    "                            format=\"%(asctime)s %(levelname)s %(threadName)s %(name)s %(message)s\",\n",
    "                            filename='bassacquirefunc.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nybass_url = 'https://www.nybass.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/home/desbrium/Metis/FishingSpots/Data/NyBass/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acquire Thread Name, Replies, Views, and Link in Angler's Cove section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(nybass_url + '/forums/')\n",
    "# page = response.text\n",
    "# soup = BeautifulSoup(page,'lxml')\n",
    "\n",
    "# Anglers_Dict = {'Thread_Name' : [], 'Thread_Replies': [], 'Thread_Views': [], 'Thread_Link' : []}\n",
    "\n",
    "# Anglers_Cove = soup.find_all('div', class_ = 'block')[1]\n",
    "# Anglers_Threads = Anglers_Cove.find_all('div', class_ = 'node-body')\n",
    "\n",
    "# for thread in Anglers_Threads:\n",
    "    \n",
    "#     Anglers_Dict['Thread_Name'].append(thread.find('a').text)\n",
    "#     Anglers_Dict['Thread_Replies'].append(thread.find('div', class_ = 'node-replies').text.strip())\n",
    "#     Anglers_Dict['Thread_Views'].append(thread.find('div', class_ = 'node-views').text.strip())\n",
    "#     Anglers_Dict['Thread_Link'].append(thread.find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f'NyBassThreadLinks{datetime.date.today().strftime(\"%m-%d-%y\")}.pkl'\n",
    "\n",
    "# file_path = os.path.join(data_folder,file_name)\n",
    "\n",
    "# f = open(file_path,\"wb\")\n",
    "# pickle.dump(Anglers_Dict,f, pickle.HIGHEST_PROTOCOL)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"rb\") as read_file:\n",
    "        \n",
    "        Anglers_Dict = pickle.load(read_file)\n",
    "    \n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access each thread link to find page links in each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anglers_PageLDict = {'Thread_Name': [], 'Page_Links':[]}\n",
    "\n",
    "# link_indexes = len(Anglers_Dict['Thread_Link'])\n",
    "# thread_links = Anglers_Dict['Thread_Link']\n",
    "# thread_names = Anglers_Dict['Thread_Name']\n",
    "\n",
    "# for index in range(link_indexes):\n",
    "    \n",
    "#     thread_link = thread_links[index]\n",
    "#     thread_name = thread_names[index]\n",
    "    \n",
    "#     response = requests.get(f'{nybass_url + thread_link}')\n",
    "#     page = response.text\n",
    "#     soup = BeautifulSoup(page,'lxml')\n",
    "    \n",
    "#     try: \n",
    "        \n",
    "#         last_page = soup.find_all('li', class_ = 'pageNav-page')[-1].text\n",
    "    \n",
    "#     except:\n",
    "        \n",
    "#         continue \n",
    "        \n",
    "#     for page_num in range(1,int(last_page)+1):\n",
    "        \n",
    "#         if thread_link not in Anglers_PageLDict['Page_Links']:\n",
    "            \n",
    "#             Anglers_PageLDict['Page_Links'].append(thread_link)\n",
    "#             Anglers_PageLDict['Thread_Name'].append(thread_name)\n",
    "        \n",
    "#         else:\n",
    "            \n",
    "#             Anglers_PageLDict['Page_Links'].append(thread_link+f'page-{page_num}')\n",
    "#             Anglers_PageLDict['Thread_Name'].append(thread_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f'NyBassPageLinks.pkl{datetime.date.today().strftime(\"%m-%d-%y\")}'\n",
    "\n",
    "# file_path = os.path.join(data_folder,file_name)\n",
    "\n",
    "# f = open(file_path,\"wb\")\n",
    "# pickle.dump(Anglers_PageLDict,f, pickle.HIGHEST_PROTOCOL)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"rb\") as read_file:\n",
    "        \n",
    "        Anglers_PageLDict = pickle.load(read_file)\n",
    "    \n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Go through each page link and acquire the post links, date , title, views, and replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anglers_PostLDict = {'Thread_Name': [], 'Page_Links': [], 'Post_Title': [], 'Post_Links': [], 'Post_Date': [], 'Post_Views': [], 'Post_Replies': []}\n",
    "\n",
    "# link_indexes = len(Anglers_PageLDict['Page_Links'])\n",
    "# page_links = Anglers_PageLDict['Page_Links']\n",
    "# thread_names = Anglers_PageLDict['Thread_Name']\n",
    "\n",
    "# for index in range(link_indexes):\n",
    "    \n",
    "#     page_link = page_links[index]\n",
    "#     thread_name = thread_names[index]\n",
    "    \n",
    "#     response = requests.get(f'{nybass_url + page_link}')\n",
    "#     page = response.text\n",
    "#     soup = BeautifulSoup(page,'lxml')\n",
    "    \n",
    "#     table = soup.find('div', class_ ='structItemContainer')\n",
    "#     post_containers = table.find_all('li', class_ = 'structItem-startDate')\n",
    "#     title_containers = table.find_all('div', class_ = 'structItem-title')\n",
    "#     view_containers = table.find_all('div', class_ = 'view-count')\n",
    "#     reply_containers = table.find_all('div', class_ = 'reply-count')\n",
    "    \n",
    "#     for post in post_containers:\n",
    "        \n",
    "#         Anglers_PostLDict['Thread_Name'].append(thread_name)\n",
    "#         Anglers_PostLDict['Page_Links'].append(page_link)\n",
    "#         Anglers_PostLDict['Post_Links'].append(post.find('a').get('href'))\n",
    "#         Anglers_PostLDict['Post_Date'].append(post.find('time').get('title'))\n",
    "    \n",
    "#     Anglers_PostLDict['Post_Title'].extend([title.text.strip() for title in title_containers])\n",
    "#     Anglers_PostLDict['Post_Views'].extend([view.text.strip() for view in view_containers])\n",
    "#     Anglers_PostLDict['Post_Replies'].extend([reply.text.strip() for reply in reply_containers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f'NyBassPostLinks.pkl{datetime.date.today().strftime(\"%m-%d-%y\")}'\n",
    "\n",
    "# file_path = os.path.join(data_folder,file_name)\n",
    "\n",
    "# f = open(file_path,\"wb\")\n",
    "# pickle.dump(Anglers_PostLDict,f, pickle.HIGHEST_PROTOCOL)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"rb\") as read_file:\n",
    "        \n",
    "        Anglers_PostLDict = pickle.load(read_file)\n",
    "    \n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_post_info(base_url=nybass_url, post_link=None):\n",
    "    \n",
    "    post_dict = {'Post_Links':'', 'Post_Content': ''}\n",
    "    page_links = []\n",
    "    page_links.append(f'{base_url + post_link}')\n",
    "    response = requests.get(page_links[0])\n",
    "    \n",
    "    if int(response.status_code) < 400:        \n",
    "        page = response.text\n",
    "        soup = BeautifulSoup(page,'lxml')\n",
    "        table = soup.find('div', class_ = 'p-body-inner')\n",
    "        post_dict['Post_Links'] += post_link\n",
    "\n",
    "        try: \n",
    "            last_page_num = int(table.find_all('li', class_ = 'pageNav-page')[-1].text)\n",
    "            if last_page_num > 2:\n",
    "                for page_num in range(2, last_page_num + 1):\n",
    "                    page_links.append(f'{base_url + post_link}' + f'page-{page_num}')\n",
    "            else: \n",
    "                page_links.append(f'{base_url + post_link}' + f'page-2')\n",
    "\n",
    "        except:\n",
    "            #logging.error(f'{page_link}{post}')\n",
    "\n",
    "        for page_link in page_links:\n",
    "            response = requests.get(page_link)\n",
    "            page = response.text\n",
    "            soup = BeautifulSoup(page,'lxml')\n",
    "            table = soup.find('div', class_ = 'p-body-inner')\n",
    "\n",
    "            try:\n",
    "                for blockquotes in table.find_all('blockquote'):\n",
    "                    blockquotes.decompose()\n",
    "            except:\n",
    "                print(page_link)\n",
    "\n",
    "            posts = table.find_all('div', class_ = 'message-cell message-cell--main california-message-cell')\n",
    "\n",
    "            for post in posts:\n",
    "                post_text = post.find('div', class_ = 'bbWrapper').text.strip()\n",
    "                #logging.error(f'{page_link}{post}')\n",
    "\n",
    "                if len(post_text) > 30:\n",
    "                    post_time = post.find('time').get('title')\n",
    "                    post_dict['Post_Content'] += post_text + ' ' + post_time + '.'\n",
    "    else:     \n",
    "        print(f'{response.status_code}')\n",
    "    \n",
    "    return post_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nybass_df(post_links = Anglers_PostLDict['Post_Links'], start_num = None, end_num = None, download_folder = data_folder):\n",
    "    \n",
    "    forum_info = []\n",
    "    \n",
    "    for link in post_links[start_num : end_num]:    \n",
    "        forum_info.append(acquire_post_info(post_link = link))\n",
    "    \n",
    "    df = pd.DataFrame(forum_info)\n",
    "    file_name = f'NYBassPostData{start_num}to{end_num}on{datetime.date.today().strftime(\"%m-%d-%y\")}.pkl'\n",
    "    file_path = os.path.join(data_folder,file_name)\n",
    "    df.to_pickle(file_path)\n",
    "    \n",
    "    logging.error(f'{start_num}{end_num}')\n",
    "    \n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_chunks(post_links = Anglers_PostLDict['Post_Links']):\n",
    "    \n",
    "    post_chunks = []\n",
    "    \n",
    "    length = len(post_links)\n",
    "    post_bins = np.linspace(0, length, num= length//40, dtype=np.int64)\n",
    "    \n",
    "    for idx, post_bin in enumerate(post_bins):\n",
    "        if idx == len(post_bins) - 1: \n",
    "            break\n",
    "        start_num =  post_bins[idx]\n",
    "        end_num = post_bins[idx+1]\n",
    "        post_chunks.append((start_num, end_num))\n",
    "        \n",
    "    return post_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post_info(post_links = Anglers_PostLDict['Post_Links'], chunk_list = post_chunks(), start_index = None):\n",
    "        \n",
    "        for index,chunk in enumerate(chunk_list[start_index:]):  \n",
    "            start_num, end_num = chunk\n",
    "            yield create_nybass_df(start_num = start_num, end_num = end_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_list = []\n",
    "\n",
    "# gen = create_post_info()\n",
    "\n",
    "# for postinfo in gen:\n",
    "#     data_list.append(postinfo)\n",
    "#     time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = []\n",
    "\n",
    "# for data in data_list:\n",
    "#     df_list.append(pd.read_pickle(data))\n",
    "\n",
    "# df = pd.concat(df_list, axis = 0)\n",
    "# nybass_df = pd.DataFrame(Anglers_PostLDict).merge(pd.DataFrame(Anglers_Dict), how = 'left', on = 'Thread_Name').merge(df, how = 'left', on = 'Post_Links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f'NYBassPostData{datetime.date.today().strftime(\"%m-%d-%y\")}.csv'\n",
    "# file_path = os.path.join(data_folder,file_name)\n",
    "# nybass_df.to_csv(file_path, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'/home/desbrium/Metis/FishingSpots/Data/NyBass/NYBassPostData06-21-21.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
